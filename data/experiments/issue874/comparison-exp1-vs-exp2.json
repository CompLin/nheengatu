{
  "alpha": 0.05,
  "test": "Mannâ€“Whitney U (two-sided)",
  "folds": 10,
  "exp1_file": "exp1/raw-text-results.json",
  "exp2_file": "exp2/raw-text-results.json",
  "PART1_EXP1_better": [],
  "PART2_EXP2_better": [],
  "PART3_no_significant_difference": [
    {
      "metric": "Parsing sum / LAS",
      "mean_exp1": 68.73,
      "mean_exp2": 69.563,
      "U": 36.0,
      "p": 0.307489
    },
    {
      "metric": "Parsing sum / UAS",
      "mean_exp1": 74.461,
      "mean_exp2": 74.796,
      "U": 44.0,
      "p": 0.677585
    },
    {
      "metric": "Tagging sum / feats",
      "mean_exp1": 86.769,
      "mean_exp2": 85.971,
      "U": 67.0,
      "p": 0.212294
    },
    {
      "metric": "Tagging sum / lemmas",
      "mean_exp1": 91.474,
      "mean_exp2": 90.886,
      "U": 58.0,
      "p": 0.570606
    },
    {
      "metric": "Tagging sum / upostag",
      "mean_exp1": 90.012,
      "mean_exp2": 89.517,
      "U": 66.0,
      "p": 0.241322
    },
    {
      "metric": "Tagging sum / xpostag",
      "mean_exp1": 89.182,
      "mean_exp2": 88.73,
      "U": 60.0,
      "p": 0.472509
    },
    {
      "metric": "Tokenizer multiword sum / f1",
      "mean_exp1": 85.437,
      "mean_exp2": 87.695,
      "U": 38.0,
      "p": 0.384136
    },
    {
      "metric": "Tokenizer sentences sum / f1",
      "mean_exp1": 59.348,
      "mean_exp2": 62.546,
      "U": 34.0,
      "p": 0.241322
    },
    {
      "metric": "Tokenizer tokens sum / f1",
      "mean_exp1": 94.643,
      "mean_exp2": 94.243,
      "U": 66.0,
      "p": 0.241322
    },
    {
      "metric": "Tokenizer words sum / f1",
      "mean_exp1": 94.415,
      "mean_exp2": 94.085,
      "U": 60.5,
      "p": 0.449521
    }
  ]
}